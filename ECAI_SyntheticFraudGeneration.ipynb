{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ECAI_SyntheticFraudGeneration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UtqHY3M4oz5"
      },
      "source": [
        "Implementation for \"Limitations and Applicability of GANs in Banking Domain\"\n",
        "\n",
        "\n",
        "http://ceur-ws.org/Vol-2692/paper1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKohoaNJzWeD"
      },
      "source": [
        "# !nvcc --version\n",
        "# !python --version\n",
        "# !pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "# !pip3 install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9XOU-ou2jqN"
      },
      "source": [
        "# #To install UMAP\n",
        "!pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsvD0fG1_Vm3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import xgboost as xgb\n",
        "import sklearn.cluster as cluster\n",
        "from sklearn.preprocessing import normalize \n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import recall_score, precision_score, roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from torchvision import datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.autograd as autograd\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "from random import Random\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import umap\n",
        "# import umap.plot\n",
        "import progressbar\n",
        "from time import sleep\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0Gc01zIwX60"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kP_ER2j-Ahx"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/CreditCardFraudDataset/train_df.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/CreditCardFraudDataset/test_df.csv')\n",
        "\n",
        "print(train_df.shape)\n",
        "train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]\n",
        "print(train_df.columns)\n",
        "print(train_df.groupby('Class')['Class'].count())\n",
        "\n",
        "test_df = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]\n",
        "print(test_df.columns)\n",
        "print(test_df.groupby('Class')['Class'].count())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEIS8zJrxYXZ"
      },
      "source": [
        "**Model Parameters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kgta_0m2RL"
      },
      "source": [
        "#Define Model Parameters\n",
        "n_clusters = 1  #no of clusters for k-means, if >1 conditional GAN = True else conditional GAN = False\n",
        "rand_dim = 30  #noise dimension\n",
        "base_n_count = 128  #no of neurons \n",
        "nb_steps = 3000 + 1 #no of iterations\n",
        "batch_size = 64\n",
        "k_d = 5   #updates for discriminator per generator update\n",
        "log_interval = 100  #epochs to show results\n",
        "learning_rate = 2e-4 \n",
        "b1 = 0.5 \n",
        "b2 = 0.9\n",
        "LAMBDA = 10.0\n",
        "loss_fn = 'None' #Options:['None', 'Triplet', 'Siamese']\n",
        "use_network = False #use triplet network? \n",
        "show = True #print the plots of generated samples and real samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "614kY-Xk-1Cj"
      },
      "source": [
        "#k_means clustering to set labels for fraud data in train set\n",
        "def k_means(train_df, k):\n",
        "  train = train_df.loc[ train_df['Class']==1 ].copy()\n",
        "  label_cols = [ i for i in train.columns if 'Class' in i ]\n",
        "  data_cols = [ i for i in train.columns if i not in label_cols ]\n",
        "  train_no_label = train[ data_cols ]\n",
        "  args = ()\n",
        "  kwds = {'n_clusters':k, 'random_state':0} \n",
        "  labels = cluster.KMeans(*args, **kwds).fit_predict(train_no_label) \n",
        "  fraud_w_classes = train.copy()     \n",
        "  fraud_w_classes['Class'] = labels  \n",
        "  return fraud_w_classes\n",
        "\n",
        "fraud_w_classes = k_means(train_df, n_clusters)\n",
        "train = fraud_w_classes.copy().reset_index(drop=True)\n",
        "\n",
        "label_cols = [ i for i in train.columns if 'Class' in i ]\n",
        "data_cols = [ i for i in train.columns if i not in label_cols ]\n",
        "train_no_label = train[ data_cols ]\n",
        "data_dim = len(data_cols)\n",
        "total_samples = 2*len(train)  #no of fraud samples to generate to augment the train set\n",
        "\n",
        "#true for conditional WGAN i.e. WCGAN and false for WGAN\n",
        "if n_clusters == 1:\n",
        "  condition = False\n",
        "  label_dim = 0\n",
        "else:\n",
        "  condition = True\n",
        "  label_dim = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3MG9OYO7Xns"
      },
      "source": [
        "**Utility Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF4TmVj0_xIj"
      },
      "source": [
        "#Split the dataset into Train and Test Set\n",
        "\n",
        "def split_dataset(data):\n",
        "  n_real = np.sum(data['Class']==0) # 200000\n",
        "  n_fraud = np.sum(data['Class']==1) # 492\n",
        "  real_samples = data.loc[ data.Class==0].sample(n_real, replace=False).reset_index(drop=True)\n",
        "  fraud_samples = data.loc[ data.Class==1].sample(n_fraud, replace=False).reset_index(drop=True)\n",
        "\n",
        "  train_fraction = 0.7\n",
        "  fn_real = int(n_real * train_fraction)\n",
        "  fn_fraud = int(n_fraud * train_fraction)\n",
        "\n",
        "  train_df = pd.concat([real_samples[:fn_real],fraud_samples[:fn_fraud]],axis=0,ignore_index=True).reset_index(drop=True)\n",
        "  test_df = pd.concat([real_samples[fn_real:],fraud_samples[fn_fraud:]],axis=0,ignore_index=True).reset_index(drop=True)\n",
        "\n",
        "  print(train_df.groupby('Class')['Class'].count())\n",
        "  print(test_df.groupby('Class')['Class'].count())\n",
        "\n",
        "  return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkp9wWklSyFr"
      },
      "source": [
        "def BaseMetrics(y_pred,y_true):\n",
        "    TP = np.sum( (y_pred == 1) & (y_true == 1) )\n",
        "    TN = np.sum( (y_pred == 0) & (y_true == 0) )\n",
        "    FP = np.sum( (y_pred == 1) & (y_true == 0) )\n",
        "    FN = np.sum( (y_pred == 0) & (y_true == 1) )\n",
        "    return TP, TN, FP, FN\n",
        "\n",
        "def SimpleMetrics(y_pred,y_true, show=True):\n",
        "    TP, TN, FP, FN = BaseMetrics(y_pred,y_true)\n",
        "    ACC = ( TP + TN ) / ( TP + TN + FP + FN )\n",
        "\n",
        "    Recall = TP / (TP+FN)\n",
        "    Precision = TP / (TP+FP)\n",
        "    \n",
        "    # Reporting\n",
        "    if show:\n",
        "      from IPython.display import display\n",
        "      print( 'Confusion Matrix')\n",
        "      display( pd.DataFrame( [[TN,FP],[FN,TP]], columns=['Pred 0','Pred 1'], index=['True 0', 'True 1'] ) )\n",
        "    \n",
        "    # print( 'Accuracy : {}'.format( ACC ))\n",
        "    # print( 'Recall  : {}'.format( Recall ))\n",
        "    # print( 'Precision  : {}'.format( Precision ))\n",
        "    return ACC, Recall, Precision\n",
        "    \n",
        "def SimpleAccuracy(y_pred,y_true):\n",
        "    TP, TN, FP, FN = BaseMetrics(y_pred,y_true)\n",
        "    ACC = ( TP + TN ) / ( TP + TN + FP + FN )\n",
        "    return ACC    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUtFCGn8zDYd"
      },
      "source": [
        "def draw_umap_test(train_df,test_df, y_pred, n_neighbors=50, min_dist=0.3, n_components=2, metric='euclidean', title=''):\n",
        "\n",
        "  np.random.seed(42)\n",
        "\n",
        "  df_tmp_tst = pd.DataFrame()\n",
        "  df_tmp_tst = test_df.copy()\n",
        "  X_col = df_tmp_tst.columns[:-1]\n",
        "  y_col = df_tmp_tst.columns[-1]\n",
        "  df_tmp_tst['y_pred'] = np.round(y_pred)\n",
        "\n",
        "  test_non_fraud = test_df.loc[test_df['Class']==0][X_col].values  \n",
        "  Random(4).shuffle(test_non_fraud)\n",
        "  test_non_fraud = test_non_fraud[:5000]\n",
        "  test_non_fraud_labels = test_df.loc[test_df['Class']==0 ]['Class'].values\n",
        "  test_non_fraud_labels = test_non_fraud_labels[:5000]\n",
        "\n",
        "  test_fraud_tp = df_tmp_tst[(df_tmp_tst['y_pred']==1)&(df_tmp_tst['Class']==1)].copy()\n",
        "  test_fraud_fn = df_tmp_tst[(df_tmp_tst['y_pred']==0)&(df_tmp_tst['Class']==1)].copy()\n",
        "\n",
        "  train_fraud = train_df[(train_df['Class']==1)][X_col].values\n",
        "\n",
        "  print(test_fraud_tp[X_col].shape, test_fraud_fn[X_col].shape, train_fraud.shape)\n",
        "\n",
        "  dtrain = np.vstack( [  test_fraud_tp[X_col], test_fraud_fn[X_col], train_fraud ]) #test_non_fraud,\n",
        "  dlabels = np.hstack( [ np.zeros(len(test_fraud_tp[y_col])), np.ones(int(len(test_fraud_fn))) , 2*np.ones(int(len(train_fraud))) ] ) #test_non_fraud_labels\n",
        "\n",
        "  \n",
        "  fit = umap.UMAP(random_state=42 )  \n",
        "  u = fit.fit_transform(dtrain)\n",
        "\n",
        "  #Train Fraud\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  labels = ['TruePositive_TestFraud', 'FalseNegative_TestFraud','TrainFraud']\n",
        "  x = u[:,0]\n",
        "  y = u[:,1]\n",
        "  for g in np.unique(dlabels):\n",
        "    if g == 1 or g == 2:\n",
        "      continue\n",
        "    i = np.where(dlabels == g)\n",
        "    ax.scatter(x[i], y[i], c=[sns.color_palette()[int(x)] for x in dlabels[i]], label=labels[int(g)])\n",
        "  ax.legend()\n",
        "  plt.title(title, fontsize=18)\n",
        "  plt.show() \n",
        "\n",
        "  #Trained Fraud and True Positives\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  labels = ['TruePositive_TestFraud', 'FalseNegative_TestFraud','TrainFraud']\n",
        "  x = u[:,0]\n",
        "  y = u[:,1]\n",
        "  for g in np.unique(dlabels):\n",
        "    if g == 2:\n",
        "      continue\n",
        "    i = np.where(dlabels == g)\n",
        "    ax.scatter(x[i], y[i], c=[sns.color_palette()[int(x)] for x in dlabels[i]], label=labels[int(g)])\n",
        "  ax.legend()\n",
        "  plt.title(title, fontsize=18)\n",
        "  plt.show()   \n",
        "\n",
        "  #True Positives, False positives and Train Fraud\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  labels = ['TruePositive_TestFraud', 'FalseNegative_TestFraud','TrainFraud']\n",
        "  x = u[:,0]\n",
        "  y = u[:,1]\n",
        "  for g in np.unique(dlabels):\n",
        "    i = np.where(dlabels == g)\n",
        "    ax.scatter(x[i], y[i], c=[sns.color_palette()[int(x)] for x in dlabels[i]], label=labels[int(g)])\n",
        "  ax.legend()\n",
        "  plt.title(title, fontsize=18)\n",
        "  plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAEkzbMO3bfP"
      },
      "source": [
        "def draw_umap(train_df,g_z, n_neighbors=50, min_dist=0.3, n_components=2, metric='euclidean', title=''):\n",
        "\n",
        "  X_col = train_df.columns[:-1]\n",
        "  y_col = train_df.columns[-1]  \n",
        "\n",
        "  train_non_fraud = train_df.loc[ train_df['Class']==0 ][X_col].values  \n",
        "  Random(4).shuffle(train_non_fraud)  \n",
        "  train_non_fraud = train_non_fraud[:10000]\n",
        "  train_non_fraud_labels = train_df.loc[ train_df['Class']==0 ][y_col].values\n",
        "  train_fraud = train_df.loc[ train_df['Class']==1 ].copy()\n",
        "\n",
        "  dtrain = np.vstack( [ train_fraud[X_col], train_non_fraud, g_z ] )\n",
        "  dlabels = np.hstack( [train_fraud[y_col], train_non_fraud_labels[:10000], 2*np.ones(int(len(g_z))) ] ) \n",
        "\n",
        "  fit = umap.UMAP(  random_state=42 )  \n",
        "  u = fit.fit_transform(dtrain)\n",
        "\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  labels = ['TrainNonFraud','TrainRealFraud', 'TrainGeneratedFraud']\n",
        "  x = u[:,0]\n",
        "  y = u[:,1]\n",
        "  for g in np.unique(dlabels):\n",
        "    i = np.where(dlabels == g)\n",
        "    ax.scatter(x[i], y[i], c=[sns.color_palette()[int(x)] for x in dlabels[i]], label=labels[int(g)])\n",
        "  ax.legend()\n",
        "  plt.title(title, fontsize=18)\n",
        "  plt.show() \n",
        "\n",
        "\n",
        "  dtrain = np.vstack( [ train_fraud[X_col], g_z ] )\n",
        "  dlabels = np.hstack( [train_fraud[y_col], np.zeros(int(len(g_z))) ] ) \n",
        "  fit = umap.UMAP(  random_state=42 )  \n",
        "  u = fit.fit_transform(dtrain)\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111)\n",
        "  labels = ['TrainGeneratedFraud', 'TrainRealFraud']\n",
        "  x = u[:,0]\n",
        "  y = u[:,1]\n",
        "  for g in np.unique(dlabels):\n",
        "    i = np.where(dlabels == g)\n",
        "    ax.scatter(x[i], y[i], c=[sns.color_palette()[int(x)] for x in dlabels[i]], label=labels[int(g)])\n",
        "  ax.legend()\n",
        "  plt.title(title, fontsize=18)\n",
        "  plt.show()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReiL2iXgS2_Y"
      },
      "source": [
        "**Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxUbQluVFpXU"
      },
      "source": [
        "#To check performance of XGBoost on the generated samples\n",
        "\n",
        "def runXGB(train_df, test_df, g_z=[]):  \n",
        "  X_col = test_df.columns[:-1]\n",
        "  y_col = test_df.columns[-1]\n",
        "\n",
        "  if len(g_z) == 0:\n",
        "    dtrain = xgb.DMatrix(train_df[X_col], train_df[y_col], feature_names=X_col)\n",
        "  else:\n",
        "    dtrain = np.vstack( [train_df[X_col], g_z] )\n",
        "    dlabels = np.hstack( [ train_df[y_col], np.ones(int(len(g_z))) ] ) \n",
        "    dtrain = xgb.DMatrix(dtrain, dlabels, feature_names=X_col)\n",
        "\n",
        "  dtest = xgb.DMatrix(test_df[X_col], test_df[y_col], feature_names=X_col)\n",
        "  y_true = test_df['Class'].values  \n",
        "  \n",
        "  xgb_params = {\n",
        "      'max_depth': 4, \n",
        "      'objective': 'binary:logistic',\n",
        "      'random_state': 0,\n",
        "      'eval_metric': 'auc',\n",
        "      }\n",
        "\n",
        "  xgb_test = xgb.train(xgb_params, dtrain, num_boost_round=10)\n",
        "\n",
        "  y_pred = xgb_test.predict(dtest, ntree_limit=xgb_test.best_iteration+1)\n",
        "\n",
        "  if show:\n",
        "    draw_umap_test(train_df,test_df, y_pred)\n",
        "\n",
        "  ACC, Recall, Precision = SimpleMetrics( np.round(y_pred), y_true)\n",
        "\n",
        "  # print( 'Accuracy : {}'.format( ACC ))\n",
        "  # print( 'Recall  : {}'.format( Recall ))\n",
        "  # print( 'Precision  : {}'.format( Precision ))\n",
        "\n",
        "  precision, recall, thresholds = metrics.precision_recall_curve(dtest.get_label(), y_pred)\n",
        "  f1 = metrics.f1_score(dtest.get_label(), np.round(y_pred))\n",
        "  # print(\"f1 Score: \",f1)   \n",
        "  pr_auc = metrics.auc(recall[:], precision[:])\n",
        "  # print('pr_auc score',pr_auc)  \n",
        "\n",
        "  print( \"[Recall: %f] [Precision: %f] [F1_score: %f] [PR_AUC: %f]\" % (Recall, Precision, f1, pr_auc))  \n",
        "\n",
        "  return y_pred, Precision, Recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlDeiiHT5CTo"
      },
      "source": [
        "#To run XGBoost on the real and generated fraud samples. Ideally the accuracy should be 50%\n",
        "# i.e not able to distinguish between real and generated fraud samples.\n",
        "\n",
        "def CheckAccuracy( x, g_z, data_cols, label_cols=[]):\n",
        "    n = len(x)\n",
        "    m = int(len(x)/2)\n",
        "    \n",
        "    #train set\n",
        "    dtrain = np.vstack( [ x[:m], g_z[:m] ] ) # train classifier on half of the samples from real and generated train fraud data\n",
        "    dlabels = np.hstack( [ np.zeros(m), np.ones(m) ] ) # label '0' for real and '1' for generated fraud samples\n",
        "    \n",
        "    #test set\n",
        "    dtest = np.vstack( [ x[int(len(x)/2):], g_z[int(len(g_z)/2):] ] ) # test the classifier on the the other half of each set\n",
        "    y_true = np.hstack( [ np.zeros(n-m), np.ones(n-m) ] )\n",
        "    \n",
        "    dtrain = xgb.DMatrix(dtrain, dlabels, feature_names=data_cols+label_cols)\n",
        "    dtest = xgb.DMatrix(dtest, feature_names=data_cols+label_cols)\n",
        "    \n",
        "    xgb_params = {\n",
        "        'max_depth': 4, \n",
        "        'objective': 'binary:logistic',\n",
        "        'random_state': 0,\n",
        "        'eval_metric': 'auc',\n",
        "        }\n",
        "    xgb_test = xgb.train(xgb_params, dtrain, num_boost_round=10) \n",
        "\n",
        "    y_pred = np.round(xgb_test.predict(dtest))\n",
        "\n",
        "    return SimpleAccuracy(y_pred, y_true)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_scgRd8dS2Oc"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()\n",
        "        self.lin1 = nn.Linear(data_dim,base_n_count*4)\n",
        "        self.lin2 = nn.Linear(base_n_count*4, base_n_count*2)\n",
        "        self.lin3 = nn.Linear(base_n_count*2, base_n_count*1)\n",
        "        self.lin4 = nn.Linear(base_n_count*1, 2)          \n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, inp):      \n",
        "      h0 = self.lin1(inp)\n",
        "      h0=self.relu(h0)\n",
        "      h0 = F.tanh(h0)\n",
        "      h1=self.lin2(h0)\n",
        "      h1=self.relu(h1)\n",
        "      # h1 = F.tanh(h1)\n",
        "      h2=self.lin3(h1)\n",
        "      h2=self.relu(h2)      \n",
        "      # h2 = F.tanh(h2)\n",
        "      h3=self.lin4(h2)   \n",
        "      return h3\n",
        "\n",
        "    def predict(self,x):\n",
        "        pred = F.softmax(self.forward(x))\n",
        "        ans = []\n",
        "        #Pick the class with maximum weight\n",
        "        for t in pred:\n",
        "            if t[0]>t[1]:\n",
        "                ans.append(0)\n",
        "            else:\n",
        "                ans.append(1)\n",
        "        return torch.tensor(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mesCRiBjoAqO"
      },
      "source": [
        "#MLP Classifier on the test dataset without oversampling(data augmentation)\n",
        "def runMLP(train_df, test_df, g_z=[], epoch = 2000):  \n",
        "  FloatTensor = torch.cuda.FloatTensor\n",
        "  LongTensor = torch.cuda.LongTensor\n",
        "\n",
        "  X_col = test_df.columns[:-1]\n",
        "  y_col = test_df.columns[-1]\n",
        "\n",
        "  if len(g_z) != 0:\n",
        "    dtrain = np.vstack( [train_df[X_col].values, g_z] )\n",
        "    dlabels = np.hstack( [ train_df[y_col].values, np.ones(int(len(g_z))) ] ) \n",
        "  else:\n",
        "    dtrain = train_df[X_col].values\n",
        "    dlabels = train_df['Class'].values  \n",
        "\n",
        "  dtrain=(torch.from_numpy(dtrain)).type(FloatTensor)\n",
        "  dlabels=(torch.from_numpy(dlabels)).type(LongTensor)\n",
        "  train_data = TensorDataset(dtrain,dlabels)\n",
        "  train_loader = DataLoader(train_data,batch_size=32,shuffle=True)      \n",
        "\n",
        "  dtest = test_df[X_col].values\n",
        "  y_true = test_df['Class'].values    \n",
        "  dtest=(torch.from_numpy(dtest)).type(FloatTensor)\n",
        "      \n",
        "  classifier = MLP()\n",
        "  auxiliary_loss = torch.nn.CrossEntropyLoss()\n",
        "  classifier.cuda()\n",
        "  auxiliary_loss.cuda()\n",
        "\n",
        "  cls_optim = torch.optim.Adam(classifier.parameters(), lr=1e-3, betas=(0.9, 0.999),weight_decay=1e-3)\n",
        "  cls_schedular=StepLR(cls_optim,step_size=200,gamma=0.9)\n",
        "\n",
        "  best_f1 = 0.0\n",
        "  best_pr_auc = 0.0\n",
        "  best_precision = 0.0\n",
        "  best_recall = 0.0\n",
        "  best_epoch = 0\n",
        "  best_y = []\n",
        "\n",
        "  bar = progressbar.ProgressBar(maxval=epoch, widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "  bar.start()\n",
        "  for it in range(epoch):\n",
        "      cls_schedular.step()\n",
        "      cls_optim.zero_grad()\n",
        "\n",
        "      x,l = train_loader.__iter__().next()      \n",
        "      \n",
        "      cls_out=classifier(x)\n",
        "      c_loss=auxiliary_loss(cls_out, l)\n",
        "      c_loss.backward()\n",
        "      cls_optim.step()\n",
        "\n",
        "      if it%100 == 0:      \n",
        "        with torch.no_grad():\n",
        "            y_pred=classifier.predict(dtest)\n",
        "        y_pred=np.array(y_pred)                                  \n",
        "            \n",
        "        ACC, Recall, Precision = SimpleMetrics( np.round(y_pred), y_true, False)\n",
        "        precision, recall, thresholds = metrics.precision_recall_curve(y_true, y_pred)\n",
        "        f1 = metrics.f1_score(y_true, np.round(y_pred))\n",
        "        pr_auc = metrics.auc(recall[:], precision[:])\n",
        "        if f1 > best_f1:\n",
        "          best_epoch = it\n",
        "          best_f1 = f1\n",
        "          best_pr_auc = pr_auc\n",
        "          best_precision = Precision\n",
        "          best_recall = Recall                    \n",
        "          best_y = y_pred\n",
        "\n",
        "        bar.update(it+1)\n",
        "        sleep(0.1)\n",
        "  bar.finish()\n",
        "  _,_,_ = SimpleMetrics( np.round(best_y), y_true)\n",
        "  print( \"[Recall: %f] [Precision: %f] [F1_score: %f] [PR_AUC: %f]\" % (best_recall,  best_precision, best_f1, best_pr_auc))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ-9Jh6W-lPK"
      },
      "source": [
        "**WGAN-GP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzZOqkAO9wF1"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Generator, self).__init__()\n",
        "      # Generator\n",
        "      self.lin1 = nn.Linear(rand_dim+label_dim,base_n_count)\n",
        "      self.lin2 = nn.Linear(base_n_count, base_n_count*2)\n",
        "      self.lin3 = nn.Linear(base_n_count*2, base_n_count*4)\n",
        "      self.lin4 = nn.Linear(base_n_count*4, data_dim)                     \n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def modelG(self,inp):\n",
        "      h0=self.lin1(inp)\n",
        "      h0=self.relu(h0)\n",
        "      h1=self.lin2(h0)\n",
        "      h1=self.relu(h1)\n",
        "      h2=self.lin3(h1)\n",
        "      h2=self.relu(h2)\n",
        "      h3=self.lin4(h2)        \n",
        "      return h3\n",
        "\n",
        "    def forward(self, inp):            \n",
        "      _x = self.modelG(inp)\n",
        "      #return torch.cat([_x,label], -1)\n",
        "      return _x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZ-XccKuDN1a"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Discriminator, self).__init__()\n",
        "      # Discriminator\n",
        "      self.lin1 = nn.Linear(data_dim+label_dim,base_n_count*4)\n",
        "      self.lin2 = nn.Linear(base_n_count*4, base_n_count*2)\n",
        "      self.lin3 = nn.Linear(base_n_count*2, base_n_count*1)\n",
        "      self.lin4 = nn.Linear(base_n_count*1, 1)    \n",
        "      self.relu = nn.ReLU()           \n",
        "\n",
        "    def modelD(self,inp):\n",
        "      h0 = self.lin1(inp)\n",
        "      h0=self.relu(h0)\n",
        "      h1=self.lin2(h0)\n",
        "      h1=self.relu(h1)\n",
        "      h2=self.lin3(h1)\n",
        "      h2=self.relu(h2)\n",
        "      h3=self.lin4(h2)        \n",
        "      return h3\n",
        "\n",
        "    def forward(self, inp):      \n",
        "      x = self.modelD(inp)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyfYUsG_wkZ6"
      },
      "source": [
        "***Gradient Penalty***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmkYAX9TSJTS"
      },
      "source": [
        "def calc_gradient_penalty(netD, real_data, fake_data,label):\n",
        "    alpha = torch.rand(batch_size, 1)\n",
        "    alpha = alpha.expand(real_data.size())\n",
        "    alpha = alpha.cuda() if cuda else alpha\n",
        "\n",
        "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
        "\n",
        "    if cuda:\n",
        "        interpolates = interpolates.cuda()\n",
        "\n",
        "    if condition:\n",
        "      interpolates = torch.cat([interpolates, label],-1)\n",
        "          \n",
        "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
        "    \n",
        "    disc_interpolates = netD(interpolates)\n",
        "\n",
        "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
        "                              grad_outputs=torch.ones(disc_interpolates.size()).cuda() if cuda else torch.ones(\n",
        "                                  disc_interpolates.size()),\n",
        "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
        "    return gradient_penalty"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6GdVd2Tn2kA"
      },
      "source": [
        "**Discriminator Rejection Sampling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxYMQBbolPOx"
      },
      "source": [
        "\n",
        "def sigmoid(F):\n",
        "    return 1/(1 + np.exp(-F))\n",
        "\n",
        "def sample():\n",
        "  # print('Start BurnIn')\n",
        "  max_M = 0.0\n",
        "  max_logit = 0.0\n",
        "  processed_samples = 0\n",
        "\n",
        "  burnin_samples = data.shape[0]*2  #no of samples to decide on the max_logit\n",
        "\n",
        "  while processed_samples<burnin_samples:\n",
        "\n",
        "    x,l = next(iter(train_loader))\n",
        "    l = Variable(l.type(Tensor))    \n",
        "    noiseIP = np.random.normal(size=[batch_size , rand_dim])\n",
        "\n",
        "    if condition:      \n",
        "      noiseIP = torch.from_numpy(noiseIP).cuda().float()      \n",
        "      z_input = torch.cat([noiseIP,l], -1)\n",
        "    else:\n",
        "      z_input = torch.from_numpy(noiseIP).cuda().float()      \n",
        "\n",
        "    with torch.no_grad():\n",
        "      g_z = generator.modelG(z_input)    \n",
        "      if condition:\n",
        "        discrim_logits = discriminator.modelD(torch.cat([g_z,l], -1))\n",
        "      else:\n",
        "        discrim_logits = discriminator.modelD(g_z)\n",
        "\n",
        "    logits = discrim_logits.view(-1).cpu().float()\n",
        "\n",
        "    batch_ratio = np.exp(logits)\n",
        "    max_idx = np.argmax(batch_ratio)\n",
        "    max_ratio = batch_ratio[max_idx]\n",
        "\n",
        "    if max_ratio > max_M:\n",
        "        max_M = max_ratio\n",
        "        max_logit = logits[max_idx]    \n",
        "\n",
        "    processed_samples += batch_size\n",
        "    # print(\"Processing BurnIn...%d/%d\"%(processed_samples, burnin_samples))        \n",
        "  # print(max_M, max_logit)\n",
        "\n",
        "  # Sample\n",
        "  gen_samples = []\n",
        "  label_samples = []\n",
        "  gen_samples1 = []\n",
        "  label_samples1 = []\n",
        "  # print (\"Start Sampling...\")\n",
        "  counter = 0\n",
        "  rejected_counter = 0\n",
        "  epsilon = 1e-8\n",
        "  gamma_percentile = 0.80\n",
        "\n",
        "  while counter < total_samples:\n",
        "    x,l = next(iter(train_loader))     \n",
        "    l = Variable(l.type(Tensor))   \n",
        "\n",
        "    if condition:\n",
        "      l = torch.tensor(np.hstack([np.zeros(1500),np.ones(1500), 2*np.ones(1500), 3*np.ones(1500), 4*np.ones(1500), 5*np.ones(1500) ]))\n",
        "      l = l.unsqueeze(-1)      \n",
        "      l = Variable(l.type(Tensor))         \n",
        "      noiseIP = np.random.normal(size=[9000 , rand_dim])\n",
        "      noiseIP = torch.from_numpy(noiseIP).cuda().float()            \n",
        "      z_input = torch.cat([noiseIP,l], -1)\n",
        "    else:\n",
        "      noiseIP = np.random.normal(size=[batch_size , rand_dim])\n",
        "      z_input = torch.from_numpy(noiseIP).cuda().float()\n",
        "\n",
        "    with torch.no_grad():                            \n",
        "      g_z = generator.modelG(z_input)\n",
        "      if condition:\n",
        "        discrim_logits = discriminator.modelD(torch.cat([g_z,l], -1))\n",
        "      else:\n",
        "        discrim_logits = discriminator.modelD(g_z)\n",
        "\n",
        "    logits = discrim_logits.view(-1).cpu().float()\n",
        "\n",
        "    batch_ratio = np.exp(logits)\n",
        "    max_idx = np.argmax(batch_ratio)\n",
        "    max_ratio = batch_ratio[max_idx]        \n",
        "\n",
        "    #update max_M if larger M is found\n",
        "    if max_ratio > max_M:\n",
        "        max_M = max_ratio\n",
        "        max_logit = logits[max_idx]\n",
        "\n",
        "    #calculate F_hat and pass it into sigmoid\n",
        "    # set gamma dynamically (95th percentile of F)\n",
        "    Fs = logits - max_logit - np.log(1 - np.exp(logits - max_logit - epsilon))\n",
        "    gamma = np.percentile(Fs, gamma_percentile)\n",
        "    F_hat = Fs - gamma\n",
        "    acceptance_prob = sigmoid(F_hat)      \n",
        "    \n",
        "    g_z = g_z.cpu().float()\n",
        "\n",
        "    l = l.cpu()\n",
        "    for idx, sample in enumerate(g_z):\n",
        "        probability = np.random.uniform(0, 1)\n",
        "        if probability <= acceptance_prob[idx]:\n",
        "          gen_samples.append(sample) \n",
        "          label_samples.append(l[idx])         \n",
        "          counter += 1\n",
        "  gen_samples = torch.stack(gen_samples,0)\n",
        "  label_samples = torch.stack(label_samples,0)\n",
        "\n",
        "  print('----------------------------After Rejection Sampling---------------------------------------')\n",
        "  print (\"Sampled Synthetic Fraud: %d\"%(counter))\n",
        "\n",
        "  #Check Classifier's Performance after data augmentation\n",
        "  runXGB(train_df, test_df, gen_samples)\n",
        "\n",
        "  if show:\n",
        "    draw_umap(train_df, gen_samples)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQxBnpa3ww3E"
      },
      "source": [
        "**Triplet Loss**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hkUXfV3wpIX"
      },
      "source": [
        "class Triplet(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Triplet, self).__init__()\n",
        "      # Generator\n",
        "      self.lin1 = nn.Linear(data_dim, 10)\n",
        "      self.lin2 = nn.Linear(10, 2)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def modelT(self,inp):\n",
        "      h0=self.lin1(inp)\n",
        "      h0=self.relu(h0)\n",
        "      h1=self.lin2(h0)        \n",
        "      return h1\n",
        "\n",
        "    def forward(self, inp1, inp2, inp3):            \n",
        "      o1 = self.modelT(inp1)\n",
        "      o2 = self.modelT(inp2)\n",
        "      o3 = self.modelT(inp3)\n",
        "      return o1, o2, o3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8v2NyHQwso9"
      },
      "source": [
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = 1\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
        "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
        "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
        "        return losses.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmIGrbP_xIP9"
      },
      "source": [
        "**Siamese**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgDKTejuxLqO"
      },
      "source": [
        "class Siamese(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Siamese, self).__init__()\n",
        "      # Generator\n",
        "      self.lin1 = nn.Linear(data_dim, data_dim)\n",
        "      self.lin2 = nn.Linear(data_dim, 2)\n",
        "      self.relu = nn.ReLU()\n",
        "\n",
        "    def modelS(self,inp):\n",
        "      h0=self.lin1(inp)\n",
        "      h0=self.relu(h0)\n",
        "      h1=self.lin2(h0)        \n",
        "      return h1\n",
        "\n",
        "    def forward(self, inp1, inp2):            \n",
        "      o1 = self.modelS(inp1)\n",
        "      o2 = self.modelS(inp2)\n",
        "      return o1, o2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKfXWjQRxVR8"
      },
      "source": [
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = 1\n",
        "        self.eps = 1e-9\n",
        "\n",
        "    def forward(self, output1, output2, target):\n",
        "        distances = (output2 - output1).pow(2).sum(1)  # squared distances\n",
        "        losses = 0.5 * (target.float() * distances + (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n",
        "        return losses.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5MVCC0zMgxQ"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDdZ_-8ZoNrW"
      },
      "source": [
        "##Classifier performance without data augmentation\n",
        "runXGB(train_df, test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-G_zp1fjGvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b15058a7-1dba-4aed-b2a8-fe5a0977bb46"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "allc = [0,1]\n",
        "class_weights = compute_class_weight('balanced', allc, train_df[label_cols].values.flatten())\n",
        "# d_class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "for i in range(len(class_weights)):\n",
        "      if (class_weights[i]) < 1:\n",
        "            class_weights[i] = 1\n",
        "\n",
        "class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1.        , 289.77325581])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSLNyGXYRh1x"
      },
      "source": [
        "# Initialize generator and discriminator and classifier\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "if loss_fn == 'Triplet':\n",
        "  loss_triplet = TripletLoss()\n",
        "  triplet = Triplet()\n",
        "if loss_fn == 'Siamese':  \n",
        "  loss_sia = ContrastiveLoss()\n",
        "  siamese = Siamese()\n",
        "\n",
        "classifier = MLP()\n",
        "class_weights=torch.FloatTensor(class_weights).cuda()\n",
        "auxiliary_loss = torch.nn.CrossEntropyLoss(class_weights)\n",
        "\n",
        "if cuda:\n",
        "    generator.cuda()\n",
        "    discriminator.cuda()\n",
        "    classifier.cuda()\n",
        "    auxiliary_loss.cuda()  \n",
        "    \n",
        "    if loss_fn == 'Triplet':\n",
        "      loss_triplet.cuda()\n",
        "      triplet.cuda()\n",
        "    if loss_fn == 'Siamese':  \n",
        "      loss_sia.cuda()\n",
        "      siamese.cuda()    \n",
        "\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "if loss_fn == 'Triplet':\n",
        "  optimizer_T = torch.optim.Adam(triplet.parameters(), lr=learning_rate, betas=(b1, b2))\n",
        "if loss_fn == 'Siamese':  \n",
        "  optimizer_S = torch.optim.Adam(list(siamese.parameters())+list(generator.parameters()), lr=learning_rate, betas=(b1, b2))\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor \n",
        "\n",
        "cls_optim = torch.optim.Adam(list(classifier.parameters())+list(generator.parameters()), lr=1e-3, betas=(0.9, 0.999),weight_decay=1e-3)\n",
        "cls_schedular=StepLR(cls_optim,step_size=200,gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BB15KwIhxwa0"
      },
      "source": [
        "#dataloader for training\n",
        "data = torch.tensor(train[data_cols].values)\n",
        "label = torch.tensor(train[label_cols].values)\n",
        "train_data = TensorDataset(data,label)\n",
        "train_loader = DataLoader(train_data,batch_size,shuffle=True) \n",
        "\n",
        "real_data = torch.tensor(train_df.loc[ train_df['Class']==0][data_cols].values)\n",
        "real_label = torch.tensor(train_df.loc[ train_df['Class']==0][label_cols].values)\n",
        "real_samples = TensorDataset(real_data, real_label)\n",
        "real_loader = DataLoader(real_samples,batch_size,shuffle=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMi-JRgQcLtX"
      },
      "source": [
        "for epoch in range(nb_steps): \n",
        "  # (1) Update D network\n",
        "  for p in discriminator.parameters():  # reset requires_grad\n",
        "    p.requires_grad = True  # they are set to False below in netG update\n",
        "\n",
        "  for iter_d in range(k_d):\n",
        "    x,l = next(iter(train_loader))\n",
        "    x = Variable(x.type(Tensor))          \n",
        "    l = Variable(l.type(Tensor))\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (x.shape[0], rand_dim))))\n",
        "\n",
        "    if condition:    \n",
        "      _x = generator(torch.cat([z,l], -1)).detach()\n",
        "    else:\n",
        "      _x = generator(z).detach()\n",
        "\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    if condition:\n",
        "      D_real = torch.mean(discriminator(torch.cat([x,l], -1)))\n",
        "      D_fake = torch.mean(discriminator(torch.cat([_x,l], -1)))\n",
        "\n",
        "    else:\n",
        "      D_real = torch.mean(discriminator(x))\n",
        "      D_fake = torch.mean(discriminator(_x))\n",
        "\n",
        "    #WGAN    \n",
        "    gradient_penalty = calc_gradient_penalty(discriminator, x.data, _x.data, l)\n",
        "    D_cost = D_fake - D_real + gradient_penalty\n",
        "    Wasserstein_D = D_real - D_fake\n",
        "\n",
        "    D_cost.backward()\n",
        "    optimizer_D.step()          \n",
        "\n",
        "  for p in discriminator.parameters():\n",
        "      p.requires_grad = False  # to avoid gradient computation\n",
        "\n",
        "  # (2) train siamese/triplet network  \n",
        "  x,l = next(iter(train_loader))\n",
        "  x = Variable(x.type(Tensor))          \n",
        "  l = Variable(l.type(Tensor))\n",
        "  z = Variable(Tensor(np.random.normal(0, 1, (x.shape[0], rand_dim))))\n",
        "\n",
        "  y,_ = next(iter(real_loader))\n",
        "  y = Variable(y.type(Tensor)) \n",
        "\n",
        "  if condition:    \n",
        "    _x = generator(torch.cat([z,l], -1)).detach()\n",
        "  else:\n",
        "    _x = generator(z).detach()  \n",
        "\n",
        "  if loss_fn=='Triplet' and use_network == True:    \n",
        "    optimizer_T.zero_grad()      \n",
        "    #triplet loss\n",
        "    o1,o2,o3 = triplet(_x,x,y)\n",
        "    T_loss = loss_triplet(o1,o2,o3)\n",
        "    T_loss.backward()\n",
        "    optimizer_T.step()\n",
        "\n",
        "  if loss_fn=='Siamese' and epoch > 1000:    \n",
        "    optimizer_S.zero_grad()      \n",
        "    #Siamese loss\n",
        "    o1,o2 = siamese(_x,x)\n",
        "    S_loss = loss_sia(o1.cpu(),o2.cpu(), torch.ones(x.size()[0]))\n",
        "    o1, o2 = siamese(_x, y)\n",
        "    S_loss += loss_sia(o1.cpu(),o2.cpu(), torch.zeros(x.size()[0]))\n",
        "    S_loss.backward()\n",
        "    optimizer_S.step()\n",
        "\n",
        "  # (3) Update classifier network  \n",
        "  if epoch < 0:\n",
        "    FloatTensor = torch.cuda.FloatTensor\n",
        "    LongTensor = torch.cuda.LongTensor  \n",
        "    dtrain = np.vstack( [x.cpu(), _x.cpu(), y.cpu()] )\n",
        "    dlabels = np.hstack( [ np.ones(int(len(x))+int(len(_x))), np.zeros(int(len(y))) ] )\n",
        "    # dtrain = np.vstack( [_x.cpu(), y.cpu()] )\n",
        "    # dlabels = np.hstack( [ np.ones(int(len(_x))), np.zeros(int(len(y))) ] )    \n",
        "    dtrain=(torch.from_numpy(dtrain)).type(FloatTensor)\n",
        "    dlabels=(torch.from_numpy(dlabels)).type(LongTensor)  \n",
        "\n",
        "    cls_schedular.step()\n",
        "    cls_optim.zero_grad()  \n",
        "    cls_out=classifier(dtrain)\n",
        "    c_loss=auxiliary_loss(cls_out, dlabels)\n",
        "    c_loss.backward()\n",
        "    cls_optim.step()         \n",
        "\n",
        "  # (4) Update G network  \n",
        "  generator.zero_grad()      \n",
        "\n",
        "  if condition:\n",
        "    _x = generator(torch.cat([z,l],-1))\n",
        "    D_fake = discriminator(torch.cat([_x,l],-1))\n",
        "  else:\n",
        "    _x = generator(z)\n",
        "    D_fake = discriminator(_x)\n",
        "\n",
        "  # WGAN\n",
        "  G_cost = -torch.mean( D_fake )\n",
        "\n",
        "  if loss_fn == 'Triplet' and use_network == False:\n",
        "    T_loss = loss_triplet(_x,x,y)\n",
        "    G_cost += T_loss\n",
        "\n",
        "  # #triplet loss\n",
        "  # if loss_fn == 'Triplet':\n",
        "  #   if use_network:\n",
        "  #     o1,o2,o3 = triplet(_x,x,y)\n",
        "  #     T_loss = loss_triplet(o1,o2,o3)\n",
        "  #   else:      \n",
        "  #     T_loss = loss_triplet(_x,x,y)\n",
        "  #   G_cost += T_loss\n",
        "\n",
        "  # #siamese loss\n",
        "  # if loss_fn == 'Siamese':\n",
        "  #   o1,o2 = siamese(_x,x)\n",
        "  #   S_loss = loss_sia(o1.cpu(),o2.cpu(), torch.ones(x.size()[0]))\n",
        "  #   o1, o2 = siamese(_x, y)\n",
        "  #   S_loss += loss_sia(o1.cpu(),o2.cpu(), torch.zeros(x.size()[0]))\n",
        "  #   S_loss = S_loss.cuda()\n",
        "  #   G_cost += 1.0 * S_loss\n",
        "\n",
        "  G_cost.backward()  \n",
        "  optimizer_G.step()          \n",
        "\n",
        "  if epoch % log_interval == 0:\n",
        "    print(\"========================================================================================================\")\n",
        "    print(\n",
        "        \"[Epoch %d/%d] [D loss: %f] [Wasserstein_D loss: %f] [G loss: %f]\" % (epoch, nb_steps, D_cost.item(),  Wasserstein_D.item(), G_cost.item())\n",
        "    )          \n",
        "    #check the quality of generated samples\n",
        "    sample()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}